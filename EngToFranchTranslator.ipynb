{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70b69618-cdf1-466b-bc7b-8d9b93f2b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import spacy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_texts, tgt_texts, src_tokenizer, tgt_tokenizer, src_vocab, tgt_vocab):\n",
    "        self.src_texts = src_texts\n",
    "        self.tgt_texts = tgt_texts\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src_texts[idx]\n",
    "        tgt_text = self.tgt_texts[idx]\n",
    "        src_tokens = [self.src_vocab.get(token.text, self.src_vocab[\"<unk>\"]) for token in self.src_tokenizer(src_text)]\n",
    "        tgt_tokens = [self.tgt_vocab.get(token.text, self.tgt_vocab[\"<unk>\"]) for token in self.tgt_tokenizer(tgt_text)]\n",
    "        return torch.tensor(src_tokens), torch.tensor(tgt_tokens)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        return self.attention(query, key, value, attn_mask=mask)[0]\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=ff_dim, batch_first=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_key_padding_mask=mask)\n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, num_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=ff_dim, batch_first=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask=tgt_mask, memory_key_padding_mask=memory_mask)\n",
    "        return tgt\n",
    "\n",
    "class TransformerTranslator(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, n_heads, ff_dim, num_layers):\n",
    "        super(TransformerTranslator, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.encoder = TransformerEncoder(d_model, n_heads, ff_dim, num_layers)\n",
    "        self.decoder = TransformerDecoder(d_model, n_heads, ff_dim, num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src = self.positional_encoding(self.src_embedding(src))\n",
    "        tgt = self.positional_encoding(self.tgt_embedding(tgt))\n",
    "        memory = self.encoder(src, mask=src_mask)\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=src_mask)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_padded = pad_sequence([torch.cat((torch.tensor([1]), seq, torch.tensor([2]))) for seq in src_batch],\n",
    "                              batch_first=True, padding_value=0)\n",
    "    tgt_padded = pad_sequence([torch.cat((torch.tensor([1]), seq, torch.tensor([2]))) for seq in tgt_batch],\n",
    "                              batch_first=True, padding_value=0)\n",
    "    max_len_src = src_padded.size(1)\n",
    "    max_len_tgt = tgt_padded.size(1)\n",
    "\n",
    "    # Adjust masks for padding\n",
    "    src_padding_mask = (src_padded == 0)\n",
    "    tgt_padding_mask = (tgt_padded == 0)\n",
    "\n",
    "    return src_padded, tgt_padded, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones((sz, sz), device='cpu'), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    return mask\n",
    "\n",
    "# Saving model weights\n",
    "def save_model(model, file_path):\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "    print(f\"Model weights saved to {file_path}\")\n",
    "\n",
    "# Loading model weights\n",
    "def load_model(model, file_path):\n",
    "    model.load_state_dict(torch.load(file_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"Model weights loaded from {file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b53aa8a1-5fc5-45d9-80a4-283780d1e146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(model, sentence, src_vocab, tgt_vocab, nlp_src, max_len=50):\n",
    "    model.eval()\n",
    "    \n",
    "    src_tokens = [src_vocab.get(token.text, src_vocab[\"<unk>\"]) for token in nlp_src(sentence)]\n",
    "    src_tensor = torch.tensor([src_tokens]).to(device)\n",
    "    \n",
    "    src_mask = None\n",
    "    memory = model.encoder(model.positional_encoding(model.src_embedding(src_tensor)), src_mask)\n",
    "    \n",
    "    tgt_tokens = [tgt_vocab[\"<sos>\"]]\n",
    "    for _ in range(max_len):\n",
    "        tgt_tensor = torch.tensor([tgt_tokens]).to(device)\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_tensor.size(1)).to(device)\n",
    "        tgt_positional = model.positional_encoding(model.tgt_embedding(tgt_tensor))\n",
    "        output = model.decoder(tgt_positional, memory, tgt_mask=tgt_mask, memory_mask=src_mask)\n",
    "        output = model.fc_out(output)\n",
    "        next_token = output.argmax(-1)[:, -1].item()\n",
    "        tgt_tokens.append(next_token)\n",
    "        \n",
    "        if next_token == tgt_vocab[\"<eos>\"]:\n",
    "            break\n",
    "    \n",
    "    inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
    "    translated_sentence = \" \".join([inv_tgt_vocab[token] for token in tgt_tokens if token not in (0, tgt_vocab[\"<sos>\"], tgt_vocab[\"<eos>\"])])\n",
    "    return translated_sentence\n",
    "\n",
    "\n",
    "# Data preparation\n",
    "nlp_src = spacy.load(\"en_core_web_sm\")\n",
    "nlp_tgt = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "src_vocab = {word: idx for idx, word in enumerate([\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"] + list(nlp_src.vocab.strings))}\n",
    "tgt_vocab = {word: idx for idx, word in enumerate([\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"] + list(nlp_tgt.vocab.strings))}\n",
    "\n",
    "# Adjust the dataset to use streaming if disk space is limited\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"wmt14\", \"fr-en\", streaming=True)\n",
    "train_data = data[\"train\"]\n",
    "\n",
    "src_texts = []\n",
    "tgt_texts = []\n",
    "for example in train_data:\n",
    "    src_sentence = example[\"translation\"][\"en\"]\n",
    "    word_count = len(src_sentence.split())\n",
    "    if word_count<8:\n",
    "        src_texts.append(src_sentence)\n",
    "        tgt_texts.append(example[\"translation\"][\"fr\"])\n",
    "    if len(src_texts) >= 100000:  # Limit to 100,000 examples\n",
    "        break\n",
    "print(len(src_texts))\n",
    "\n",
    "train_dataset = TranslationDataset(src_texts, tgt_texts, nlp_src, nlp_tgt, src_vocab, tgt_vocab)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)  # Adjust batch size\n",
    "\n",
    "# Example usage\n",
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "ff_dim = 2048\n",
    "num_layers = 6\n",
    "\n",
    "model = TransformerTranslator(src_vocab_size, tgt_vocab_size, d_model, n_heads, ff_dim, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dccbc446-e7aa-45cb-9bd5-d0de6fc065e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Desk\\AppData\\Local\\Temp\\ipykernel_12652\\454531719.py:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(file_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded from transformer_weights.pth\n",
      "Translated: Les États-Unis évoquées mondial évoquées mondial évoquées mondial évoquées dramatique sensible leur -je Lamy mondial gérer leur -je Lamy mondial -je Lamy mondial évoquées dramatique leur -je Lamy mondial -je Lamy -je Lamy mondial -je Lamy mondial -je Lamy -je Lamy mondial -je Lamy mondial -je Lamy mondial -je Lamy\n"
     ]
    }
   ],
   "source": [
    "file_path=\"transformer_weights.pth\"\n",
    "\n",
    "load_model(model, file_path)\n",
    "\n",
    "test_sentence = \"Hello world\"\n",
    "translated = translate_sentence(model, test_sentence, src_vocab, tgt_vocab, nlp_src)\n",
    "print(\"Translated:\", translated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a88d12c2-823c-4dc1-81ec-a330b550945e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated: <unk> monde <unk> <unk> <unk> <unk> les <unk> <unk> à <unk> monde plus <unk> <unk> <unk> les deux actualité actualité actualité actualité <unk> plus <unk> <unk> <unk> <unk> d' actualité monde <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "print(\"Translated:\", translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f22aa04f-a81b-4943-bdc1-e51cf592d2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch time 106.11256885528564 mSec , Loss: 0.3247288465499878\n",
      "batch time 103.96519589424133 mSec , Loss: 0.481305867433548\n",
      "batch time 103.47373080253601 mSec , Loss: 0.5381783843040466\n",
      "Epoch 1, Loss: 0.4056829810142517\n",
      "Weights saved to transformer_weights.pth\n",
      "Translated: Les États-Unis : Autre programme Autre programme : les réseaux : leur propre leur propre touche à leur propre touche à leur propre touche à la famine en Lamy : Lamy : leur rends par leur propre leur rends Lamy : Lamy à leur rends Lamy : Lamy : Lamy\n",
      "batch time 106.54590964317322 mSec , Loss: 0.3038889467716217\n",
      "batch time 102.06413555145264 mSec , Loss: 0.4543510377407074\n",
      "batch time 103.74764585494995 mSec , Loss: 0.3406851887702942\n",
      "Epoch 2, Loss: 0.3881976306438446\n",
      "batch time 103.97550845146179 mSec , Loss: 0.2372269183397293\n",
      "batch time 104.35857677459717 mSec , Loss: 0.2705625295639038\n",
      "batch time 103.89511132240295 mSec , Loss: 0.2870049476623535\n",
      "Epoch 3, Loss: 0.23062372207641602\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "import time\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    count=0\n",
    "    mtime=0\n",
    "    for src, tgt, src_padding_mask, tgt_padding_mask in train_loader:\n",
    "        \n",
    "        count+=1\n",
    "        start_time = time.time()\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        src_padding_mask, tgt_padding_mask = src_padding_mask.to(device), tgt_padding_mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:].reshape(-1)\n",
    "        \n",
    "        src_mask = None\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "        \n",
    "        \n",
    "        output = model(src, tgt_input, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        output = output.reshape(-1, tgt_vocab_size)\n",
    "        \n",
    "        loss = criterion(output, tgt_output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        \n",
    "        # Очистка GPU памяти\n",
    "        torch.cuda.empty_cache()\n",
    "        end_time= time.time()\n",
    "        mtime+=(end_time-start_time)\n",
    "        if count==1000:\n",
    "            count=0\n",
    "            b_time=mtime\n",
    "            mtime=0\n",
    "            print(f\"batch time {b_time} mSec , Loss: {loss.item()}\")\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "    \n",
    "    if epoch%3==0:\n",
    "        # Сохранение весов модели в файл\n",
    "        model_path = \"transformer_weights.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Weights saved to {model_path}\")\n",
    "        # Test translation\n",
    "        test_sentence = \"Hello world\"\n",
    "        translated = translate_sentence(model, test_sentence, src_vocab, tgt_vocab, nlp_src)\n",
    "        print(\"Translated:\", translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bbe94390-c127-41fa-a008-dc64aa99fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"transformer_weights.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa1e90b9-4b58-4918-8afa-f4154eba0aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated: Il n ' y faut que votre n ' était que votre n ' est que votre nom n n ' est que votre n ' appelle que votre nom n n n n n n ' appelle que votre n n n ' appelle que votre n n n\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"What is your name\"\n",
    "translated = translate_sentence(model, test_sentence, src_vocab, tgt_vocab, nlp_src)\n",
    "print(\"Translated:\", translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08b8a35e-25e9-41a5-bcc5-85f6eb237930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumption of the session - Reprise de la session\n",
      "Madam President, on a point of order. - Madame la Présidente, c'est une motion de procédure.\n",
      "Madam President, on a point of order. - Madame la Présidente, c'est une motion de procédure.\n",
      "It is the case of Alexander Nikitin. - Il s'agit du cas d'Alexandre Nikitin.\n",
      "Why are there no fire instructions? - Comment se fait-il qu'il n'y ait pas de consignes en cas d'incendie ?\n",
      "Why are no-smoking areas not enforced? - Comment se fait-il que l'on ne respecte pas les zones non fumeurs ?\n",
      "We do not know what is happening. - Nous ne savons pas ce qui se passe.\n",
      "Agenda - Ordre des travaux\n",
      "Relating to Wednesday: - En ce qui concerne le mercredi :\n",
      "(Applause from the PSE Group) - (Applaudissements du groupe PSE)\n",
      "We therefore respect whatever Parliament may decide. - Par conséquent, nous respectons les décisions que pourrait prendre le Parlement dans ce sens.\n",
      "(Parliament rejected the request) President. - (Le Parlement rejette la demande) La Présidente.\n",
      "Thank you, Mr Poettering. - Merci Monsieur Poettering.\n",
      "It is not a lot to ask. - Ce n' est pas demander beaucoup.\n",
      "Thank you very much. - Merci.\n",
      "My vote was \"in favour\" . - J'ai voté \"pour\".\n",
      "There is no room for amendments. - Les modifications n'ont pas lieu d'être.\n",
      "That did not happen. - Mais ma demande n'a pas été satisfaite.\n",
      "This is an important matter. - C'est important.\n",
      "You did not call me either. - Moi non plus, vous ne m'avez pas donné la parole.\n",
      "I would urge you to endorse this. - Je vous demande votre approbation.\n",
      "I congratulate him on his excellent report. - Je le félicite de son excellent rapport.\n",
      "This is a pity, in a sense. - Dans un certain sens, c'est dommage.\n",
      "This, however, does not seem feasible. - Toutefois, cela ne paraît pas réalisable.\n",
      "The debate is closed. - Le débat est clos.\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    print(f\"{src_texts[i]} - {tgt_texts[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c0849d-e341-42f2-a63f-26ac00b669ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n"
     ]
    }
   ],
   "source": [
    "print(src_texts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03ac0ad4-a4ad-4476-adc0-0180962d2515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.\n"
     ]
    }
   ],
   "source": [
    "print(tgt_texts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49bc6577-bdd4-40d8-b4b6-832d6f4968e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125\n",
      "11\n",
      "12\n",
      "12\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "count=0\n",
    "for src, tgt,_,o in train_loader:\n",
    "    count+=1\n",
    "    print(len(src[0]))\n",
    "    if count==10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c093ca0-14d3-4c61-947a-10a21656a79d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
